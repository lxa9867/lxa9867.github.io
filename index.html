<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?fdd598bc02396d3df4b9a622e6899078";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NXEHV2KQ9G"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'G-NXEHV2KQ9G');
</script>
<head>
<link rel="icon" type="image/png" href="images/profile.jpg">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="google-site-verification" content="Rkdj0lekZi26eVU3MTz7HbTC4-yxdhWqgSHBRiro2rw" />
<meta name="keywords" content="Xiang Li, CMU, 李希昂, ECE, CS, PhD, CMU, Carnegie Mellon University">
<meta name="description" content="profile">

<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Xiang Li</title>
<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>
<body>
<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1><font face="Arial"> Xiang Li 李希昂</font></h1>
				</div>

				<h3><font face="Arial"> Research Scientist </font></h3>
				<p><font face="Arial">
					Google DeepMind, <br>
					Kirkland, WA. <br>
					<br>
					<em>Email: <a href="mailto:choyingw@usc.edu">xangl@google.com</a></em> <br>
<!--					<a href="supp/CV.pdf"><em>[CV]</em></a>-->
					<footer class="site-footer">
						<div class="wrapper">
							<div class="footer-col">
							<a href="https://github.com/lxa9867" target="_blank">
							<img src="images/color-github.png" class="social-icon", width="40" height="40">
							</a>
<!--							<a href="https://twitter.com/ChoYingWu2" target="_blank">-->
<!--							<img src="images/color-twitter.png" class="social-icon", width="40" height="40">-->
<!--							</a>-->
							<a href="https://www.linkedin.com/in/xiang-li-40551a16a" target="_blank">
							<img src="images/color-linkedin.png" class="social-icon", width="40" height="40">
							</a>
							<a href="https://scholar.google.com/citations?hl=en&user=hGPBdf4AAAAJ" target="_blank">
							<img src="images/color-gscholar-footer.png" class="social-icon", width="40" height="40">
							</a>
							</div>
							</div>
						</div>
					</footer>  
				</font></p>
				<!--<p> <a href="https://scholar.google.com/citations?user=tUb4J0kAAAAJ&hl=en"><img src="./pic/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/xw-hu"><img src="./pic/github_s.jpg" height="20px" style="margin-bottom:-3px"></a>
					<a href="https://www.facebook.com/xiaowei.hu.102"><img src="./pic/Facebook_s.png" height="30px" style="margin-bottom:-3px"></a>
				</p> -->
			</td>
			<td>
				<img src="images/profile.jpg" border="0" width="240"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>


<p style="text-align:justify";><font face="Arial">
	I am a research scientist at Google Deepmind. I got my PhD at Carnegie Mellon University advised by <a href="http://mlsp.cs.cmu.edu/people/bhiksha/">Prof. Bhiksha Raj</a>.
          I am currently working on <a href="https://deepmind.google/models/veo/">Veo</a> project. My research interest lies on the representation learning for media generation and understanding.

<!-- 	<font color="FF0000">I am helping my PhD advisor to look for highly motivated students interested in control/audio/image tokenizers (having personal computes is priortized)! Send an email to discuss. </font> -->


<!--	Research interests: Multimodal Learning, Image/Video Synthesis, Video Segmentation. <br><br>-->


<!--    <p><strong><font face="Arial", color=#519299> Internship:</font></strong></p>-->

<!--	<style>-->
<!--		.ImageHolder{-->
<!--		text-align:center;-->
<!--		}-->

<!--		.Image{-->
<!--		display:inline-block;-->
<!--		margin-right: 80px;-->
<!--		margin-bottom: 5px;-->
<!--		text-align:center;-->
<!--		}-->
<!--	</style>-->

<!--	<div class="Image">-->
<!--		<img src="images/Argo_AI.png" , height="110">-->
<!--		<p> <a href="https://www.argo.ai/">Argo AI</a>, 2019</p>-->
<!--	</div>-->
<!--	<div class="Image">-->
<!--		<img src="images/Amazon_126.png" , height="130">-->
<!--		<p> <a href="https://amazon.jobs/en">Amazon</a>, 2020</p>-->
<!--	</div>-->
<!--	<div class="Image">-->
<!--		<img src="images/facebook.png" , height="130">-->
<!--		<p> <a href="https://tech.fb.com/ar-vr/">Facebook</a>, 2021</p>-->
<!--	</div>-->

<!--	<div class="Image">-->
<!--		<img src="images/nv.png" , height="130">-->
<!--		<p> <a href="https://www.nvidia.com/en-us/about-nvidia/careers/">NVIDIA</a>, 2022</p>-->
<!--	</div>-->
<!--    -->
<!--</font></p>-->



<h2><font face="Arial"> News </font></h2>
<ul style="list-style-type:none">
   <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3">
	   [HEAD] DeepMind prevents me from publishing new papers :( <br>
	   [05/2025] One paper accepted to <strong>ICML 2025</strong>. <br>
	   [02/2025] One paper accepted to <strong>CVPR 2025</strong>. <br>
	   [01/2025] Two papers (one first-author) accepted to <strong>ICLR 2025</strong>. <br>
	   [09/2024] Three papers accepted to <strong>NeurIPS 2024</strong>. <br>
	   [06/2024] One first-author paper accepted to <strong>ECCV 2024</strong>. <br>
   	  [06/2024] One paper accepted to <strong>InterSpeech 2024</strong>. <br>
	   [05/2024] Two papers (one first-author) accepted to <strong>ICML 2024</strong>. <br>
	   [03/2024] One paper accepted to <strong>NAACL 2024</strong>. <br>
	  [02/2024] One first-author paper accepted to <strong>CVPR 2024</strong>. <br>
      [12/2023] One paper accepted to <strong>ICASSP 2024</strong>. <br>
      [10/2023] One first-author paper accepted to <strong>EMNLP 2023</strong>. <br>
	  [09/2023] One first-author paper accepted to <strong>NeurIPS 2023</strong>. <br>
	  [06/2023] One first-author paper accepted to <strong>ICCV 2023</strong>. <br>
	  [06/2023] One first-author paper accepted to <strong>ACM MM 2023</strong>. <br>
<!--   	  [05/2023] One co-first author paper accepted to <strong>InterSpeech 2023</strong>. <br>-->
<!--   	  [12/2022] One first-author paper accepted to <strong>AAAI 2023</strong>. <br>-->
   </font></p>
</ul>



<h2><font face="Arial"> Publications </font></h2>
<ul style="list-style-type:none">
	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/robusttok.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>Robust Latent Matters: Boosting Image Generation with Sampling Error Synthesis </b></font><br>
			<i> Kai Qiu*, <b>Xiang Li*</b>, Jason Kuen, Hao Chen, Xiaohao Xu, Jiuxiang Gu, Yinyi Luo, Bhiksha Raj, Zhe Lin, Marios Savvides</i><br>
				preprint<br>
		[<a href="https://arxiv.org/pdf/2503.08354">paper</a>]
		[<a href="https://github.com/lxa9867/ImageFolder">code</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/ambiguity-in-space.jpg" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>Towards Ambiguity-Free Spatial Foundation Model: Rethinking and Decoupling Depth Ambiguity </b></font><br>
			<i> Xiaohao Xu, Feng Xue, <b>Xiang Li</b>, Haowei Li, Shusheng Yang, Tianyi Zhang, Matthew Johnson-Roberson, Xiaonan Huang</i><br>
				preprint<br>
		[<a href="https://arxiv.org/pdf/2503.06014">paper</a>]
		[<a href="https://github.com/Xiaohao-Xu/Ambiguity-in-Space">code</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/maetok.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>Masked Autoencoders Are Effective Tokenizers for Diffusion Models </b></font><br>
			<i> Hao Chen, Yujin Han, Fangyi Chen, <b>Xiang Li</b>, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, Bhiksha Raj</i><br>
				ICML 2025<br>
		[<a href="https://arxiv.org/pdf/2502.03444">paper</a>]
		[<a href="https://github.com/Hhhhhhao/continuous_tokenizer">code</a>]
		</p> <br></td>
		</tr>
	</tbody></table>
	
	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/softvq.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>SoftVQ-VAE: Efficient 1-Dimensional Continuous Tokenizer </b></font><br>
			<i> Hao Chen, Ze Wang, <b>Xiang Li</b>, Ximeng Sun, Fangyi Chen, Jiang Liu, Jindong Wang, Bhiksha Raj, Zicheng Liu, Emad Barsoum</i><br>
				CVPR 2025<br>
		[<a href="https://arxiv.org/abs/2412.10958">paper</a>]
		[<a href="https://github.com/Hhhhhhao/continuous_tokenizer">code</a>]
		</p> <br></td>
		</tr>
	</tbody></table>
	
	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/xqgan.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>XQ-GAN: An Open-source Image Tokenization Framework for Autoregressive Generation </b></font><br>
			<i> <b>Xiang Li</b>*, Kai Qiu*, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, Zhe Lin</i><br>
				preprint<br>
		[<a href="https://arxiv.org/abs/2412.01762">paper</a>]
		[<a href="https://github.com/lxa9867/ImageFolder">code</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/synllm.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>On the Diversity of Synthetic Data and its Impact on Training Large Language Models </b></font><br>
			<i> Hao Chen, Abdul Waheed, <b>Xiang Li</b>, Yidong Wang, Jindong Wang, Bhiksha Raj, Marah I Abdin</i><br>
				preprint<br>
		[<a href="">paper</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/aar.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>Efficient Autoregressive Audio Modeling via Next-Scale Prediction </b></font><br>
			<i> Kai Qiu, <b>Xiang Li</b>, Hao Chen, Jie Sun, Jinglu Wang, Zhe Lin, Marios Savvides, Bhiksha Raj</i><br>
				preprint<br>
		[<a href="https://arxiv.org/pdf/2408.09027">paper</a>]
		[<a href="https://github.com/qiuk2/AAR">code</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/controlvar.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>ControlVAR: Exploring Controllable Visual Autoregressive Modeling </b></font><br>
			<i> <b>Xiang Li</b>, Kai Qiu, Hao Chen, Jason Kuen, Zhe Lin, Rita Singh, Bhiksha Raj</i><br>
				preprint<br>
		[<a href="https://arxiv.org/abs/2406.09750">paper</a>]
		[<a href="https://github.com/lxa9867/ControlVAR">code</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

		<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/imagefolder.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>ImageFolder: Autoregressive Image Generation with Folded Tokens </b></font><br>
			<i> <b>Xiang Li</b>, Hao Chen, Kai Qiu, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, Zhe Lin</i><br>
				ICLR 2025<br>
		[<a href="https://arxiv.org/abs/2410.01756">paper</a>]
		[<a href="https://github.com/lxa9867/ImageFolder">code</a>]
		[<a href="https://lxa9867.github.io/works/imagefolder/index.html">project page</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/3dbenchmark.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video </b></font><br>
			<i> Xiaohao Xu, Tianyi Zhang, Shibo Zhao, <b>Xiang Li</b>, Sibo Wang, Yongqi Chen, Ye Li, Bhiksha Raj, Matthew Johnson-Roberson, Sebastian Scherer, Xiaonan Huang</i><br>
				ICLR 2025<br>
		[<a href="">paper</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/imprecise.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations </b></font><br>
			<i> Hao Chen, Ankit Shah, Jindong Wang, Ran Tao, Yidong Wang, <b>Xiang Li</b>, Xing Xie, Masashi Sugiyama, Rita Singh, Bhiksha Raj</i><br>
				NeurIPS 2024<br>
		[<a href="https://arxiv.org/abs/2305.12715">paper</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/corruption.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>Slight Corruption in Pre-training Data Makes Better Diffusion Models </b></font><br>
			<i> Hao Chen, Yujin Han, Diganta Misra, <b>Xiang Li</b>, Kai Hu, Difan Zou, Masashi Sugiyama, Jindong Wang, Bhiksha Raj</i><br>
				NeurIPS 2024<br>
		[<a href="https://arxiv.org/pdf/2405.20494">paper</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/jail.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization </b></font><br>
			<i> Kai Hu, Weichen Yu, Tianjun Yao, <b>Xiang Li</b>, Wenhe Liu, Lijun Yu, Yining Li, Kai Chen, Zhiqiang Shen, Matt Fredrikson</i><br>
				NeurIPS 2024<br>
		[<a href="https://arxiv.org/abs/2405.09113">paper</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

		<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/r2bench.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>R^2-Bench: Benchmarking the Robustness of Referring Perception Models under Perturbations </b></font><br>
			<i> <b>Xiang Li</b>, Kai Qiu, Jinglu Wang, Xiaohao Xu, Rita Singh, Kashu Yamazaki, Hao Chen, Xiaonan Huang, Bhiksha Raj</i><br>
				ECCV 2024<br>
		[<a href="https://arxiv.org/abs/2403.04924">paper</a>]
		[<a href="https://github.com/lxa9867/r2bench">code</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/dmi.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>Evaluating and Improving Continual Learning in Spoken Language Understanding </b></font><br>
			<i> Muqiao Yang, <b>Xiang Li</b>, Umberto Cappellazzo, Shinji Watanabe, Bhiksha Raj</i><br>
				InterSpeech 2024<br>
		[<a href="https://arxiv.org/abs/2402.10427">paper</a>]
		</p> <br></td>
		</tr>
	</tbody></table>


	<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="30%" valign="top">
				<img src="images/maskcomp.jpg" style="border-style: none" width="100%">
			</td>
			<td width="70%" valign="top"><font color = #246B02> <b>Completing Visual Objects via Bridging Generation and Segmentation </b></font><br>
				<i> <b>Xiang Li</b>, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu</i><br>
					ICML 2024<br>
			[<a href="https://arxiv.org/abs/2310.00808">paper</a>]
		</p> <br></td>
			</tr></tbody></table>


	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/SQD.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>Rethinking Audiovisual Segmentation with Semantic Quantization and Decomposition </b></font><br>
			<i> <b>Xiang Li</b>, Jinglu Wang, Xiaohao Xu, Rita Singh, Yan Lu, Bhiksha Raj</i><br>
				CVPR 2024<br>
		[<a href="https://arxiv.org/abs/2310.00132">paper</a>]
		[<a href="https://github.com/lxa9867/QSD">code</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

		<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/autoprm.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>AutoPRM: Self-supervised Fine-grained Feedback for Multi-Step Reasoning via Controllable Question Decomposition </b></font><br>
			<i> Zhaorun Chen, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, <b>Xiang Li</b>, Bhiksha Raj, Huaxiu Yao</i><br>
				NAACL 2024 (short version at ICLR 2024 R2-FM workshop)<br>
		[<a href="https://arxiv.org/abs/2402.11452">paper</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/weak.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>A General Framework for Learning from Weak Supervision </b></font><br>
			<i> Hao Chen, Jindong Wang, Lei Feng, <b>Xiang Li</b>, Yidong Wang, Xing Xie, Masashi Sugiyama, Rita Singh, Bhiksha Raj</i><br>
				ICML 2024<br>
		[<a href="https://arxiv.org/pdf/2402.01922.pdf">paper</a>]
			</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/slam.jpg" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>Customizable Perturbation Synthesis for Robust SLAM Benchmarking </b></font><br>
			<i> Xiaohao Xu, Tianyi Zhang, Sibo Wang, <b>Xiang Li</b>, Yongqi Chen, Ye Li, Bhiksha Raj, Matthew Johnson-Roberson, Xiaonan Huang</i><br>
				preprint<br>
		[<a href="https://arxiv.org/abs/2402.08125">paper</a>]
<!--			[<a href="https://distdepth.github.io/">project page</a>]-->
			[<a href="https://github.com/Xiaohao-Xu/SLAM-under-Perturbation">code</a>]
<!--			[<a href="https://distdepth.github.io/">data</a>]-->
<!--			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]-->
<!--			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br>-->
<!--			<i>Practical indoor depth estimation: without depth annotation, efficient training data collection, high generalizability, and accurate and real-time depth sensing.</i>-->
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/csl.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>A Closer Look at Reinforcement Learning-based Automatic Speech Recognition </b></font><br>
			<i> Fan Yang, Muqiao Yang, <b>Xiang Li</b>, Yuxuan Wu, Zhiyuan Zhao, Bhiksha Raj, Rita Singh</i><br>
				Computer Speech & Language<br>
		[<a href="https://lxa9867.github.io/">paper</a>]
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/icassp_cl.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>Improving Continual Learning of Acoustic Scene Classification via Mutual Information Optimization </b></font><br>
			<i> Muqiao Yang, Umberto Cappellazzo, <b>Xiang Li</b>, Shinji Watanabe, Bhiksha Raj</i><br>
				ICASSP 2024<br>
		[<a href="https://lxa9867.github.io/">paper</a>]
<!--			[<a href="https://distdepth.github.io/">project page</a>]-->
<!--			[<a href="https://github.com/facebookresearch/DistDepth">code</a>]-->
<!--			[<a href="https://distdepth.github.io/">data</a>]-->
<!--			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]-->
<!--			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br>-->
<!--			<i>Practical indoor depth estimation: without depth annotation, efficient training data collection, high generalizability, and accurate and real-time depth sensing.</i>-->
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/STBridge.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02> <b>Towards Noise-Tolerant Speech-Referring Video Object Segmentation:
		Bridging Speech and Text </b></font><br>
			<i> <b>Xiang Li</b>, Jinglu Wang, Xiaohao Xu, Muqiao Yang, Rita Singh, Bhiksha Raj</i><br>
				EMNLP 2023<br>
		[<a href="https://aclanthology.org/2023.emnlp-main.140.pdf">paper</a>]
<!--			[<a href="https://distdepth.github.io/">project page</a>]-->
<!--			[<a href="https://github.com/facebookresearch/DistDepth">code</a>]-->
<!--			[<a href="https://distdepth.github.io/">data</a>]-->
<!--			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]-->
<!--			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br>-->
<!--			<i>Practical indoor depth estimation: without depth annotation, efficient training data collection, high generalizability, and accurate and real-time depth sensing.</i>-->
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="30%" valign="top">
				<img src="images/paintseg.png" style="border-style: none" width="100%">
			</td>
			<td width="70%" valign="top"><font color = #246B02> <b>PaintSeg: Training-free Segmentation via Painting </b></font><br>
				<i> <b>Xiang Li</b>, Chung-Ching Lin, Yinpeng Chen, Jinglu Wang, Zicheng Liu, Bhiksha Raj</i><br>
					NeurIPS 2023<br>
			[<a href="http://arxiv.org/abs/2305.19406">paper</a>]
<!--			[<a href="https://distdepth.github.io/">project page</a>]-->
			[<a href="https://github.com/lxa9867/PaintSeg">code</a>]
<!--			[<a href="https://distdepth.github.io/">data</a>]-->
<!--			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]-->
<!--			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br>-->
<!--			<i>Practical indoor depth estimation: without depth annotation, efficient training data collection, high generalizability, and accurate and real-time depth sensing.</i>-->
			</p> <br></td>
			</tr>
		</tbody></table>

	  <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="30%" valign="top">
				<img src="images/mm2023.jpg" style="border-style: none" width="100%">
			</td>
			<td width="70%" valign="top"><font color = #246B02> <b>Rethinking Voice-Face Correlation: A Geometry View </b></font><br>
				<i> <b>Xiang Li</b>, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj </i><br>
					ACM Multimedia, 2023<br>
			[<a href="pdf/Rethinking_Voice_Face_Correlation__A_Geometry_View.pdf">paper</a>]
				[<a href="https://github.com/lxa9867/VAF">code</a>]
<!--			[<a href="https://distdepth.github.io/">project page</a>]-->
<!--			[<a href="https://github.com/facebookresearch/DistDepth">code</a>]-->
<!--			[<a href="https://distdepth.github.io/">data</a>]-->
<!--			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]-->
<!--			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br>-->
<!--			<i>Practical indoor depth estimation: without depth annotation, efficient training data collection, high generalizability, and accurate and real-time depth sensing.</i>-->
			</p> <br></td>
			</tr>
		</tbody></table>

		 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="30%" valign="top">
				<img src="works/rrvos/img/teaser.jpg" style="border-style: none" width="100%">
			</td>
			<td width="70%" valign="top"><font color = #246B02><b>Robust Referring Video Object Segmentation with Cyclic Structural Consensus </b></font><br>
				<i> <b>Xiang Li</b>, Jinglu Wang, Xiaohao Xu, Xiao Li, Yan Lu, Bhiksha Raj</i><br>
				 ICCV, 2023<br>
			[<a href="https://arxiv.org/pdf/2207.01203.pdf">paper</a>]
			[<a href="works/rrvos/index.html">project page</a>]
				[<a href="https://github.com/lxa9867/R2VOS">code</a>]
<!--			[<a href="https://github.com/choyingw/GAIS-Net">code</a>]-->
<!--			[<a href="https://www.youtube.com/watch?v=QmzGAStQXOc">video</a>]<br>-->
<!--			<i> The first outdoor instacne segmentation that using disparity maps. Based on Mask-RCNN, we show that using multi-modality of geometric information can improve the performance.</i> -->
			</p> <br></td>
			</tr>
		</tbody></table>

		<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="30%" valign="top">
				<img src="images/interspeech23.jpg" style="border-style: none" width="100%">
			</td>
			<td width="70%" valign="top"><font color = #246B02> <b>The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features </b></font><br>
				<i> Liao Qu*, Xianwei Zou*, <b>Xiang Li*</b>, Wendong Yan, Rita Singh, Bhiksha Raj</i><br>
					InterSpeech, 2023<br>
			[<a href="pdf/Interspeech2023___The_Hidden_Dance_of_Phonemes_and_Visage__Unveiling_the_Enigmatic_Link_between_Phonemes_and_Facial_Features.pdf">paper</a>]
<!--			[<a href="https://distdepth.github.io/">project page</a>]-->
			[<a href="https://github.com/Oscarwasoccupied/Interspeech23_Phonemes_and_Visage/tree/main">code</a>]
<!--			[<a href="https://distdepth.github.io/">data</a>]-->
<!--			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]-->
<!--			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br>-->
<!--			<i>Practical indoor depth estimation: without depth annotation, efficient training data collection, high generalizability, and accurate and real-time depth sensing.</i>-->
			</p> <br></td>
			</tr>
		</tbody></table>

		 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="30%" valign="top">
				<img src="images/wcnc2023.jpg" style="border-style: none" width="100%">
			</td>
			<td width="70%" valign="top"><font color = #246B02> <b>Self-supervised Multi-Modal Video Forgery Attack Detection </b></font><br>
				<i>  Chenhui Zhao, <b>Xiang Li</b>, Rabhi Younes</i><br>
				WCNC, 2023 <br>
			[<a href="https://arxiv.org/pdf/2209.06345.pdf">paper</a>]
			[<a href="https://github.com/ChuiZhao/Secure-Mask">code</a>]
<!--			[<a href="works/Voice2Mesh/index.html">project page</a>]-->
<!--			[<a href="https://youtu.be/0PSyXbLw3oo">video</a>]-->
<!--			[<a href="works/Voice2Mesh/CVPR22_CMP_poster.pdf">poster</a>]<br>-->
			<!-- [<a href="https://www.youtube.com/watch?v=FQDTdpMPKxs">Youtube video</a>]<br> -->
<!--			<i>An anlaysis on the statistical correlation between voices and 3D faces. Unlike previous work using 2D representations that include background or hairstyle variations, our 3D approach better validate correlation between voices and geometry.</i> -->
			</p><br></td>
			</tr>
		</tbody></table>

		<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="30%" valign="top">
				<img src="images/pavsod.png" style="border-style: none" width="100%">
			</td>
			<td width="70%" valign="top"><font color = #246B02><b>Panoramic Video Salient Object Detection with Ambisonic Audio Guidance </b></font><br>
				<i> <b>Xiang Li</b>, Haoyuan Cao, Shijie Zhao, Junlin Li, Li Zhang, Bhiksha Raj</i><br>
				 AAAI, 2023<br>
			[<a href="https://arxiv.org/pdf/2211.14419.pdf">paper</a>]
			[<a href="https://drive.google.com/file/d/1LRWCPYT68Ziaz2D6Z8gQ-GGHHjazlgdn/view?usp=share_link">visualization</a>]
<!--			[<a href="https://github.com/choyingw/GAIS-Net">code</a>]-->
<!--			[<a href="https://www.youtube.com/watch?v=QmzGAStQXOc">video</a>]<br>-->
<!--			<i> The first outdoor instacne segmentation that using disparity maps. Based on Mask-RCNN, we show that using multi-modality of geometric information can improve the performance.</i> -->
			</p> <br></td>
			</tr>
		</tbody></table>

	 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="30%" valign="top">
				<img src="images/twc22.png" style="border-style: none" width="100%">
			</td>
			<td width="70%" valign="top"><font color = #246B02><b>Forgery Attack Detection in Surveillance Video Streams Using Wi-Fi Channel State Information </b></font><br>
				<i> Yong Huang, <b>Xiang Li</b>, Wei Wang, Tao Jiang, Qian Zhang</i><br>
				  IEEE Transactions on Wireless Communication<br>
			[<a href="https://arxiv.org/pdf/2201.09487">paper</a>]
<!--			[<a href="https://github.com/choyingw/SCADC-DepthCompletion">code</a>]-->
<!--			[<a href="works/SCADC/index.html">project page</a>]-->
<!--			[<a href="https://www.youtube.com/watch?v=FQDTdpMPKxs">1-min demo</a>]-->
<!--			[<a href="https://www.youtube.com/watch?v=IentdAL0Quk">long version video</a>]-->
<!--			[<a href="works/SCADC/img/2152-Poster.pdf">poster</a>]-->
<!--			[<a href="works/SCADC/img/2152-slides.pdf">slides</a>]<br>-->
<!--			<i>This work is the first to attend scene-completeness issue of depth completion. We obtain both structured and accurate scene depth.</i> -->
			</p> <br></td>
			</tr>
		</tbody></table>

	 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="30%" valign="top">
				<img src="images/aaai22.png" style="border-style: none" width="100%">
			</td>
			<td width="70%" valign="top"><font color = #246B02> <b>Hybrid Instance-aware Temporal Fusion for Online Video Instance Segmentation </b></font><br>
				<i>  <b>Xiang Li</b>, Jinglu Wang, Xiao Li, Yan Lu</i><br>
				AAAI, 2022 <br>
			[<a href="https://www.aaai.org/AAAI22Papers/AAAI-3675.LiX.pdf">paper</a>]
<!--			[<a href="https://github.com/choyingw/Voice2Mesh">code</a>]-->
<!--			[<a href="works/Voice2Mesh/index.html">project page</a>]-->
<!--			[<a href="https://youtu.be/0PSyXbLw3oo">video</a>]-->
<!--			[<a href="works/Voice2Mesh/CVPR22_CMP_poster.pdf">poster</a>]<br>-->
			<!-- [<a href="https://www.youtube.com/watch?v=FQDTdpMPKxs">Youtube video</a>]<br> -->
<!--			<i>An anlaysis on the statistical correlation between voices and 3D faces. Unlike previous work using 2D representations that include background or hairstyle variations, our 3D approach better validate correlation between voices and geometry.</i> -->
			</p><br></td>
			</tr>
		</tbody></table>

			 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="30%" valign="top">
				<img src="images/visif.png" style="border-style: none" width="100%">
			</td>
			<td width="70%" valign="top"><font color = #246B02><b>Video Instance Segmentation by Instance Flow Assembly </b></font><br>
				<i> <b>Xiang Li</b>, Jinglu Wang, Xiao Li, Yan Lu</i><br>
				 IEEE Transactions on Multimedia<br>
			[<a href="https://arxiv.org/pdf/2110.10599">paper</a>]
<!--			[<a href="works/GAIS-Net/index.html">project page</a>]-->
<!--			[<a href="https://github.com/choyingw/GAIS-Net">code</a>]-->
<!--			[<a href="https://www.youtube.com/watch?v=QmzGAStQXOc">video</a>]<br>-->
<!--			<i> The first outdoor instacne segmentation that using disparity maps. Based on Mask-RCNN, we show that using multi-modality of geometric information can improve the performance.</i> -->
			</p> <br></td>
			</tr>
		</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="30%" valign="top">
			<img src="images/infocom21.png" style="border-style: none" width="100%">
		</td>
		<td width="70%" valign="top"><font color = #246B02><b>Towards Cross-Modal Forgery Detection and Localization on Live Surveillance Videos </b></font><br>
			<i> Yong Huang, <b>Xiang Li</b>, Wei Wang, Tao Jiang, Qian Zhang</i><br>
			  IEEE INFOCOM, 2021 <br>
		[<a href="https://arxiv.org/pdf/2101.00848.pdf">paper</a>]
<!--		[<a href="https://github.com/choyingw/SynergyNet">code</a>]-->
<!--		[<a href="works/SynergyNet/index.html">project page</a>]-->
<!--		[<a href="https://youtu.be/i1Y8U2Z20ko">video</a>]-->
<!--		[<a href="works/SynergyNet/img/2152-Poster.pdf">poster</a>]<br>-->
<!--		<i>This work attains the <b>state of the art</b> on 3D facial geometry prediction, including 3D facial alignment, face orientation estimation, and 3D face modeling.</i> -->
		</p><br></td>
		</tr>
	</tbody></table>

	 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="30%" valign="top">
				<img src="images/sensors.png" style="border-style: none" width="100%">
			</td>
			<td width="70%" valign="top"><font color = #246B02><b>Predicting Spatial Visualization Problems’ Difficulty Level from Eye-Tracking Data </b></font><br>
				<i>  <b>Xiang Li</b>, Rabih Younes, Diana Bairaktarova, Qi Guo</i><br>
				  Sensors, 2020 <br>
			[<a href="https://www.mdpi.com/1424-8220/20/7/1949/pdf">paper</a>]
<!--			[<a href="works/GAIS-Net/index.html">project page</a>]-->
<!--			[<a href="https://github.com/choyingw/GAIS-Net">code</a>]-->
<!--			[<a href="https://www.youtube.com/watch?v=QmzGAStQXOc">video</a>]<br>-->
<!--			<i> The first outdoor instacne segmentation that using disparity maps. Based on Mask-RCNN, we show that using multi-modality of geometric information can improve the performance.</i> -->
			</p> <br></td>
			</tr>
		</tbody></table>

	 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="30%" valign="top">
				<img src="images/ubicomp20.png" style="border-style: none" width="100%">
			</td>
			<td width="70%" valign="top"><font color = #246B02><b>ActivityGAN: Generative Adversarial Networks for Data Augmentation in Sensor-Based Human Activity Recognition </b></font><br>
				<i>  <b>Xiang Li</b>, Jinqi Luo, Rabih Younes</i><br>
				  DLHAR workshop @ Ubicomp (Best Paper Award), 2020 <br>
			[<a href="https://dl.acm.org/doi/10.1145/3410530.3414367">paper</a>]
<!--			[<a href="https://github.com/ICpachong/Grid-GCN">code</a>]-->
			</p><br></td>
			</tr>
		</tbody></table>

		 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="30%" valign="top">
				<img src="images/ijcai_workshop.png" style="border-style: none" width="100%">
			</td>
			<td width="70%" valign="top"><font color = #246B02><b>Toward Data Augmentation and Interpretation in Sensor-Based Fine-Grained Hand Activity Recognition </b></font><br>
				<i> Jinqi Luo, <b>Xiang Li</b>, Rabih Younes</i><br>
				  ML4HAR workshop @ IJCAI, 2020 <br>
			[<a href="https://link.springer.com/chapter/10.1007/978-981-16-0575-8_3">paper</a>]
<!--			[<a href="https://github.com/ICpachong/Grid-GCN">code</a>]-->
			</p><br></td>
			</tr>
		</tbody></table>

	
	<!--
	<li> <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
            Advanced Optimization Approach for Low-Rank Models with Nonconvex Surrogates and Dual Momentum <br> 
         <i>   <b>Cho Ying Wu</b>, Jian Jiun Ding </i><br> 
	     <br>
	 [paper]
	 </font>
	 </p> </li>
	-->


</ul>


<h2><font face="Arial"> Academic Activities </font></h2>
<ul style="list-style-type:none">

	 <li> <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
              <strong> Reviewer </strong> <br> </font> </p>
	      <p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
		 Conference: ICML, ICLR, NeurIPS, CVPR, ICCV, ECCV, EMNLP, NAACL, ACL, AAAI, ACM MM<br>
	      </font> </p>
         </li>

	<li> <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
              <strong> Mentorship </strong> <br> </font> </p>
		<p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
		 Kai Qiu: MS at CMU (ongoing). <br>
	      </font> </p>
         </li>
		<p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
		 Liao Qu: MS at CMU. Now MLE at Tiktok.<br>
	      </font> </p>
         </li>
		<p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
		 Xianwei Zou: MS at CMU. Now PhD at UC Santa Barbara (UCSB).<br>
	      </font> </p>
         </li>
		</li>
	      <p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
		 Chenhui Zhao: MS at Duke. Now PhD at Duke University.<br>
	      </font> </p>
		 </li>
		<p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
		 Zhaorun Chen: MS at Purdue. Now PhD at University of Chicago.<br>
	      </font> </p>
         </li>
	      <p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
		 Fan Yang: MS at CMU. Now PhD at Ohio State University (OSU).<br>
	      </font> </p>
		 </li>




<!--	 <li> <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">-->
<!--              <strong> Teaching Assistant </strong> <br> </font> </p> -->
<!--	      <p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">-->
<!--		   Database Systems, University of Southern California, Spring 2022<br>-->
<!--		   Computer Graphics, University of Southern California, Fall 2021<br>-->
<!--		   Database Systems, University of Southern California, Spring 2021<br>-->
<!--	       Computer Graphics, University of Southern California, Fall 2020<br>-->
<!--	       Database Systems, University of Southern California, Spring 2020<br>-->
<!--	       Computer Graphics, University of Southern California, Fall 2019<br>-->
<!--	       Data Structures and Object Oriented Design, University of Southern California, Spring 2019<br>-->
<!--	       Advanced Digital Signal Processing, National Taiwan University, Spring 2017 <br>-->
<!--	       Differential Equation, National Taiwan University, Fall 2016 <br>-->
<!--	      </font> </p>-->
<!--	 </li>-->
	</ul>
	
<footer class="site-footer">
<br><br>
<div class="wrapper">
							<div class="footer-col">
							<a href="https://github.com/lxa9867" target="_blank">
							<img src="images/color-github.png" class="social-icon", width="40" height="40">
							</a>
<!--							<a href="https://twitter.com/ChoYingWu2" target="_blank">-->
<!--							<img src="images/color-twitter.png" class="social-icon", width="40" height="40">-->
<!--							</a>-->
							<a href="https://www.linkedin.com/in/xiang-li-40551a16a" target="_blank">
							<img src="images/color-linkedin.png" class="social-icon", width="40" height="40">
							</a>
							<a href="https://scholar.google.com/citations?hl=en&user=hGPBdf4AAAAJ" target="_blank">
							<img src="images/color-gscholar-footer.png" class="social-icon", width="40" height="40">
							</a>
							</div>
							</div>
						</div>

</footer>
		
</body></html>

