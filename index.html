<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?fdd598bc02396d3df4b9a622e6899078";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NXEHV2KQ9G"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'G-NXEHV2KQ9G');
</script>
<head>
<link rel="icon" type="image/png" href="images/profile.jpg">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="google-site-verification" content="Rkdj0lekZi26eVU3MTz7HbTC4-yxdhWqgSHBRiro2rw" />
<meta name="keywords" content="Xiang Li, CMU, 李希昂, ECE, CS, PhD, CMU, Carnegie Mellon University">
<meta name="description" content="profile">

<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Xiang Li</title>
<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>
<body>
<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1><font face="Arial"> Xiang Li 李希昂</font></h1>
				</div>

				<h3><font face="Arial"> Ph.D. Candidate </font></h3>
				<p><font face="Arial">
					Department of Electrical and Computer Engineering, <br>
					Carnegie Mellon University, <br>
					Pittsburgh, PA. <br>
					<br>
					<em>Email: <a href="mailto:choyingw@usc.edu">xl6@andrew.cmu.edu</a></em> <br>
<!--					<a href="supp/CV.pdf"><em>[CV]</em></a>-->
					<footer class="site-footer">
						<div class="wrapper">
							<div class="footer-col">
							<a href="https://github.com/lxa9867" target="_blank">
							<img src="images/color-github.png" class="social-icon", width="40" height="40">
							</a>
<!--							<a href="https://twitter.com/ChoYingWu2" target="_blank">-->
<!--							<img src="images/color-twitter.png" class="social-icon", width="40" height="40">-->
<!--							</a>-->
							<a href="https://www.linkedin.com/in/xiang-li-40551a16a" target="_blank">
							<img src="images/color-linkedin.png" class="social-icon", width="40" height="40">
							</a>
							<a href="https://scholar.google.com/citations?hl=en&user=hGPBdf4AAAAJ" target="_blank">
							<img src="images/color-gscholar-footer.png" class="social-icon", width="40" height="40">
							</a>
							</div>
							</div>
						</div>
					</footer>  
				</font></p>
				<!--<p> <a href="https://scholar.google.com/citations?user=tUb4J0kAAAAJ&hl=en"><img src="./pic/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/xw-hu"><img src="./pic/github_s.jpg" height="20px" style="margin-bottom:-3px"></a>
					<a href="https://www.facebook.com/xiaowei.hu.102"><img src="./pic/Facebook_s.png" height="30px" style="margin-bottom:-3px"></a>
				</p> -->
			</td>
			<td>
				<img src="images/profile.jpg" border="0" width="240"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>


<p style="text-align:justify";><font face="Arial">
	I am a PhD candidate at Carnegie Mellon University (CMU). I
          am advised by <a href="http://mlsp.cs.cmu.edu/people/bhiksha/">Prof. Bhiksha Raj</a>.
          My current research is about audio profiling, image/video synthesis and multimodal video segmentation.
          Before that, I spent one year at Microsoft Research, Asia working on video segmentation as a research intern. I received my B.Eng in Electrical and
          Electronic Engineering from Huazhong University of Science and Technology, where I was advised by
          <a href="http://eic.hust.edu.cn/professor/wangwei/index.html">Prof. Wei Wang</a>.<br><br>

		Research interests: <strong>Multimodal Learning</strong>, <strong>Image/Video Synthesis</strong>, <strong>Video Segmentation</strong>. <br><br>

<!--    <p><strong><font face="Arial", color=#519299> Internship:</font></strong></p>-->

<!--	<style>-->
<!--		.ImageHolder{-->
<!--		text-align:center;-->
<!--		}-->

<!--		.Image{-->
<!--		display:inline-block;-->
<!--		margin-right: 80px;-->
<!--		margin-bottom: 5px;-->
<!--		text-align:center;-->
<!--		}-->
<!--	</style>-->

<!--	<div class="Image">-->
<!--		<img src="images/Argo_AI.png" , height="110">-->
<!--		<p> <a href="https://www.argo.ai/">Argo AI</a>, 2019</p>-->
<!--	</div>-->
<!--	<div class="Image">-->
<!--		<img src="images/Amazon_126.png" , height="130">-->
<!--		<p> <a href="https://amazon.jobs/en">Amazon</a>, 2020</p>-->
<!--	</div>-->
<!--	<div class="Image">-->
<!--		<img src="images/facebook.png" , height="130">-->
<!--		<p> <a href="https://tech.fb.com/ar-vr/">Facebook</a>, 2021</p>-->
<!--	</div>-->

<!--	<div class="Image">-->
<!--		<img src="images/nv.png" , height="130">-->
<!--		<p> <a href="https://www.nvidia.com/en-us/about-nvidia/careers/">NVIDIA</a>, 2022</p>-->
<!--	</div>-->
<!--    -->
<!--</font></p>-->



<h2><font face="Arial"> News </font></h2>
<ul style="list-style-type:none">
   <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3">
	   [09/2023] I will attend NeurIPS in person, see you in New Orleans! <br>
	  [09/2023] 1 papers accepted to NeurIPS 2023 <br>
	  [06/2023] 1 paper accepted to ICCV 2023 <br>
	  [06/2023] 1 paper accepted to ACM MM 2023 <br>
   	  [05/2023] 1 paper accepted to InterSpeech 2023 <br>
   	  [12/2022] 1 paper accepted to AAAI 2023 <br>
   </font></p>
</ul>



<h2><font face="Arial"> Publications </font></h2>
<ul style="list-style-type:none">
		<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="images/maskcomp.jpg" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02> <b>Completing Visual Objects via Bridging Generation and Segmentation </b></font><br>
				<i> <b>Xiang Li</b>, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu</i><br>
					preprint<br>
			[<a href="https://lxa9867.github.io/">paper</a>]
<!--			[<a href="https://distdepth.github.io/">project page</a>]-->
<!--			[<a href="https://github.com/facebookresearch/DistDepth">code</a>]-->
<!--			[<a href="https://distdepth.github.io/">data</a>]-->
<!--			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]-->
<!--			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br>-->
<!--			<i>Practical indoor depth estimation: without depth annotation, efficient training data collection, high generalizability, and accurate and real-time depth sensing.</i>-->
			</p> <br></td>
			</tr>
		</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="40%" valign="top">
			<img src="images/SQD.png" style="border-style: none" width="100%">
		</td>
		<td width="60%" valign="top"><font color = #246B02> <b>Rethinking Audiovisual Segmentation with Semantic Quantization and Decomposition </b></font><br>
			<i> <b>Xiang Li</b>, Jinglu Wang, Xiaohao Xu, Rita Singh, Yan Lu, Bhiksha Raj</i><br>
				preprint<br>
		[<a href="https://lxa9867.github.io/">paper</a>]
<!--			[<a href="https://distdepth.github.io/">project page</a>]-->
<!--			[<a href="https://github.com/facebookresearch/DistDepth">code</a>]-->
<!--			[<a href="https://distdepth.github.io/">data</a>]-->
<!--			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]-->
<!--			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br>-->
<!--			<i>Practical indoor depth estimation: without depth annotation, efficient training data collection, high generalizability, and accurate and real-time depth sensing.</i>-->
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="40%" valign="top">
			<img src="images/STBridge.png" style="border-style: none" width="100%">
		</td>
		<td width="60%" valign="top"><font color = #246B02> <b>Towards Noise-Tolerant Speech-Referring Video Object Segmentation:
		Bridging Speech and Text </b></font><br>
			<i> <b>Xiang Li</b>, Jinglu Wang, Xiaohao Xu, Muqiao Yang, Rita Singh, Bhiksha Raj</i><br>
				preprint<br>
		[<a href="https://lxa9867.github.io/">paper</a>]
<!--			[<a href="https://distdepth.github.io/">project page</a>]-->
<!--			[<a href="https://github.com/facebookresearch/DistDepth">code</a>]-->
<!--			[<a href="https://distdepth.github.io/">data</a>]-->
<!--			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]-->
<!--			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br>-->
<!--			<i>Practical indoor depth estimation: without depth annotation, efficient training data collection, high generalizability, and accurate and real-time depth sensing.</i>-->
		</p> <br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="images/paintseg.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02> <b>PaintSeg: Training-free Segmentation via Painting </b></font><br>
				<i> <b>Xiang Li</b>, Chung-Ching Lin, Yinpeng Chen, Jinglu Wang, Zicheng Liu, Bhiksha Raj</i><br>
					NeurIPS 2023<br>
			[<a href="http://arxiv.org/abs/2305.19406">paper</a>]
<!--			[<a href="https://distdepth.github.io/">project page</a>]-->
			[<a href="https://github.com/lxa9867/PaintSeg">code (coming)</a>]
<!--			[<a href="https://distdepth.github.io/">data</a>]-->
<!--			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]-->
<!--			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br>-->
<!--			<i>Practical indoor depth estimation: without depth annotation, efficient training data collection, high generalizability, and accurate and real-time depth sensing.</i>-->
			</p> <br></td>
			</tr>
		</tbody></table>

	  <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="images/mm2023.jpg" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02> <b>Rethinking Voice-Face Correlation: A Geometry View </b></font><br>
				<i> <b>Xiang Li</b>, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj </i><br>
					ACM Multimedia, 2023<br>
			[<a href="pdf/Rethinking_Voice_Face_Correlation__A_Geometry_View.pdf">paper</a>]
				[<a href="https://github.com/lxa9867/VAF">code (coming)</a>]
<!--			[<a href="https://distdepth.github.io/">project page</a>]-->
<!--			[<a href="https://github.com/facebookresearch/DistDepth">code</a>]-->
<!--			[<a href="https://distdepth.github.io/">data</a>]-->
<!--			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]-->
<!--			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br>-->
<!--			<i>Practical indoor depth estimation: without depth annotation, efficient training data collection, high generalizability, and accurate and real-time depth sensing.</i>-->
			</p> <br></td>
			</tr>
		</tbody></table>

		 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="works/rrvos/img/teaser.jpg" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>Robust Referring Video Object Segmentation with Cyclic Structural Consensus </b></font><br>
				<i> <b>Xiang Li</b>, Jinglu Wang, Xiaohao Xu, Xiao Li, Yan Lu, Bhiksha Raj</i><br>
				 ICCV, 2023<br>
			[<a href="https://arxiv.org/pdf/2207.01203.pdf">paper</a>]
			[<a href="works/rrvos/index.html">project page</a>]
				[<a href="https://github.com/lxa9867/R2VOS">code</a>]
<!--			[<a href="https://github.com/choyingw/GAIS-Net">code</a>]-->
<!--			[<a href="https://www.youtube.com/watch?v=QmzGAStQXOc">video</a>]<br>-->
<!--			<i> The first outdoor instacne segmentation that using disparity maps. Based on Mask-RCNN, we show that using multi-modality of geometric information can improve the performance.</i> -->
			</p> <br></td>
			</tr>
		</tbody></table>

		<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="images/interspeech23.jpg" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02> <b>The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features </b></font><br>
				<i> Liao Qu*, Xianwei Zou*, <b>Xiang Li*</b>, Wendong Yan, Rita Singh, Bhiksha Raj</i><br>
					InterSpeech, 2023<br>
			[<a href="pdf/Interspeech2023___The_Hidden_Dance_of_Phonemes_and_Visage__Unveiling_the_Enigmatic_Link_between_Phonemes_and_Facial_Features.pdf">paper</a>]
<!--			[<a href="https://distdepth.github.io/">project page</a>]-->
<!--			[<a href="https://github.com/facebookresearch/DistDepth">code</a>]-->
<!--			[<a href="https://distdepth.github.io/">data</a>]-->
<!--			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]-->
<!--			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br>-->
<!--			<i>Practical indoor depth estimation: without depth annotation, efficient training data collection, high generalizability, and accurate and real-time depth sensing.</i>-->
			</p> <br></td>
			</tr>
		</tbody></table>

		 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="images/wcnc2023.jpg" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02> <b>Self-supervised Multi-Modal Video Forgery Attack Detection </b></font><br>
				<i>  Chenhui Zhao, <b>Xiang Li</b>, Rabhi Younes</i><br>
				WCNC, 2023 <br>
			[<a href="https://arxiv.org/pdf/2209.06345.pdf">paper</a>]
			[<a href="https://github.com/ChuiZhao/Secure-Mask">code</a>]
<!--			[<a href="works/Voice2Mesh/index.html">project page</a>]-->
<!--			[<a href="https://youtu.be/0PSyXbLw3oo">video</a>]-->
<!--			[<a href="works/Voice2Mesh/CVPR22_CMP_poster.pdf">poster</a>]<br>-->
			<!-- [<a href="https://www.youtube.com/watch?v=FQDTdpMPKxs">Youtube video</a>]<br> -->
<!--			<i>An anlaysis on the statistical correlation between voices and 3D faces. Unlike previous work using 2D representations that include background or hairstyle variations, our 3D approach better validate correlation between voices and geometry.</i> -->
			</p><br></td>
			</tr>
		</tbody></table>

		<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="images/pavsod.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>Panoramic Video Salient Object Detection with Ambisonic Audio Guidance </b></font><br>
				<i> <b>Xiang Li</b>, Haoyuan Cao, Shijie Zhao, Junlin Li, Li Zhang, Bhiksha Raj</i><br>
				 AAAI, 2023<br>
			[<a href="https://arxiv.org/pdf/2211.14419.pdf">paper</a>]
			[<a href="https://drive.google.com/file/d/1LRWCPYT68Ziaz2D6Z8gQ-GGHHjazlgdn/view?usp=share_link">visualization</a>]
<!--			[<a href="https://github.com/choyingw/GAIS-Net">code</a>]-->
<!--			[<a href="https://www.youtube.com/watch?v=QmzGAStQXOc">video</a>]<br>-->
<!--			<i> The first outdoor instacne segmentation that using disparity maps. Based on Mask-RCNN, we show that using multi-modality of geometric information can improve the performance.</i> -->
			</p> <br></td>
			</tr>
		</tbody></table>

	 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="images/twc22.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>Forgery Attack Detection in Surveillance Video Streams Using Wi-Fi Channel State Information </b></font><br>
				<i> Yong Huang, <b>Xiang Li</b>, Wei Wang, Tao Jiang, Qian Zhang</i><br>
				  IEEE Transactions on Wireless Communication<br>
			[<a href="https://arxiv.org/pdf/2201.09487">paper</a>]
<!--			[<a href="https://github.com/choyingw/SCADC-DepthCompletion">code</a>]-->
<!--			[<a href="works/SCADC/index.html">project page</a>]-->
<!--			[<a href="https://www.youtube.com/watch?v=FQDTdpMPKxs">1-min demo</a>]-->
<!--			[<a href="https://www.youtube.com/watch?v=IentdAL0Quk">long version video</a>]-->
<!--			[<a href="works/SCADC/img/2152-Poster.pdf">poster</a>]-->
<!--			[<a href="works/SCADC/img/2152-slides.pdf">slides</a>]<br>-->
<!--			<i>This work is the first to attend scene-completeness issue of depth completion. We obtain both structured and accurate scene depth.</i> -->
			</p> <br></td>
			</tr>
		</tbody></table>

	 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="images/aaai22.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02> <b>Hybrid Instance-aware Temporal Fusion for Online Video Instance Segmentation </b></font><br>
				<i>  <b>Xiang Li</b>, Jinglu Wang, Xiao Li, Yan Lu</i><br>
				AAAI, 2022 <br>
			[<a href="https://www.aaai.org/AAAI22Papers/AAAI-3675.LiX.pdf">paper</a>]
<!--			[<a href="https://github.com/choyingw/Voice2Mesh">code</a>]-->
<!--			[<a href="works/Voice2Mesh/index.html">project page</a>]-->
<!--			[<a href="https://youtu.be/0PSyXbLw3oo">video</a>]-->
<!--			[<a href="works/Voice2Mesh/CVPR22_CMP_poster.pdf">poster</a>]<br>-->
			<!-- [<a href="https://www.youtube.com/watch?v=FQDTdpMPKxs">Youtube video</a>]<br> -->
<!--			<i>An anlaysis on the statistical correlation between voices and 3D faces. Unlike previous work using 2D representations that include background or hairstyle variations, our 3D approach better validate correlation between voices and geometry.</i> -->
			</p><br></td>
			</tr>
		</tbody></table>

			 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="images/visif.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>Video Instance Segmentation by Instance Flow Assembly </b></font><br>
				<i> <b>Xiang Li</b>, Jinglu Wang, Xiao Li, Yan Lu</i><br>
				 IEEE Transactions on Multimedia<br>
			[<a href="https://arxiv.org/pdf/2110.10599">paper</a>]
<!--			[<a href="works/GAIS-Net/index.html">project page</a>]-->
<!--			[<a href="https://github.com/choyingw/GAIS-Net">code</a>]-->
<!--			[<a href="https://www.youtube.com/watch?v=QmzGAStQXOc">video</a>]<br>-->
<!--			<i> The first outdoor instacne segmentation that using disparity maps. Based on Mask-RCNN, we show that using multi-modality of geometric information can improve the performance.</i> -->
			</p> <br></td>
			</tr>
		</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="40%" valign="top">
			<img src="images/infocom21.png" style="border-style: none" width="100%">
		</td>
		<td width="60%" valign="top"><font color = #246B02><b>Towards Cross-Modal Forgery Detection and Localization on Live Surveillance Videos </b></font><br>
			<i> Yong Huang, <b>Xiang Li</b>, Wei Wang, Tao Jiang, Qian Zhang</i><br>
			  IEEE INFOCOM, 2021 <br>
		[<a href="https://arxiv.org/pdf/2101.00848.pdf">paper</a>]
<!--		[<a href="https://github.com/choyingw/SynergyNet">code</a>]-->
<!--		[<a href="works/SynergyNet/index.html">project page</a>]-->
<!--		[<a href="https://youtu.be/i1Y8U2Z20ko">video</a>]-->
<!--		[<a href="works/SynergyNet/img/2152-Poster.pdf">poster</a>]<br>-->
<!--		<i>This work attains the <b>state of the art</b> on 3D facial geometry prediction, including 3D facial alignment, face orientation estimation, and 3D face modeling.</i> -->
		</p><br></td>
		</tr>
	</tbody></table>

	 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="images/sensors.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>Predicting Spatial Visualization Problems’ Difficulty Level from Eye-Tracking Data </b></font><br>
				<i>  <b>Xiang Li</b>, Rabih Younes, Diana Bairaktarova, Qi Guo</i><br>
				  Sensors, 2020 <br>
			[<a href="https://www.mdpi.com/1424-8220/20/7/1949/pdf">paper</a>]
<!--			[<a href="works/GAIS-Net/index.html">project page</a>]-->
<!--			[<a href="https://github.com/choyingw/GAIS-Net">code</a>]-->
<!--			[<a href="https://www.youtube.com/watch?v=QmzGAStQXOc">video</a>]<br>-->
<!--			<i> The first outdoor instacne segmentation that using disparity maps. Based on Mask-RCNN, we show that using multi-modality of geometric information can improve the performance.</i> -->
			</p> <br></td>
			</tr>
		</tbody></table>

	 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="images/ubicomp20.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>ActivityGAN: Generative Adversarial Networks for Data Augmentation in Sensor-Based Human Activity Recognition </b></font><br>
				<i>  <b>Xiang Li</b>, Jinqi Luo, Rabih Younes</i><br>
				  DLHAR workshop @ Ubicomp (Best Paper Award), 2020 <br>
			[<a href="https://dl.acm.org/doi/10.1145/3410530.3414367">paper</a>]
<!--			[<a href="https://github.com/ICpachong/Grid-GCN">code</a>]-->
			</p><br></td>
			</tr>
		</tbody></table>

		 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="images/ijcai_workshop.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>Toward Data Augmentation and Interpretation in Sensor-Based Fine-Grained Hand Activity Recognition </b></font><br>
				<i> Jinqi Luo, <b>Xiang Li</b>, Rabih Younes</i><br>
				  ML4HAR workshop @ IJCAI, 2020 <br>
			[<a href="https://link.springer.com/chapter/10.1007/978-981-16-0575-8_3">paper</a>]
<!--			[<a href="https://github.com/ICpachong/Grid-GCN">code</a>]-->
			</p><br></td>
			</tr>
		</tbody></table>

	
	<!--
	<li> <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
            Advanced Optimization Approach for Low-Rank Models with Nonconvex Surrogates and Dual Momentum <br> 
         <i>   <b>Cho Ying Wu</b>, Jian Jiun Ding </i><br> 
	     <br>
	 [paper]
	 </font>
	 </p> </li>
	-->


</ul>


<h2><font face="Arial"> Academic Activities </font></h2>
<ul style="list-style-type:none">

	 <li> <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
              <strong> Reviewer </strong> <br> </font> </p>
	      <p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
		 Conference: ECCV 2022, AAAI 2023, 2024 ICCV 2023, EMNLP 2023<br>
	      </font> </p>
		 <p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
		 Journal: TIP<br>
	      </font> </p>
         </li>

<!--	 <li> <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">-->
<!--              <strong> Teaching Assistant </strong> <br> </font> </p> -->
<!--	      <p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">-->
<!--		   Database Systems, University of Southern California, Spring 2022<br>-->
<!--		   Computer Graphics, University of Southern California, Fall 2021<br>-->
<!--		   Database Systems, University of Southern California, Spring 2021<br>-->
<!--	       Computer Graphics, University of Southern California, Fall 2020<br>-->
<!--	       Database Systems, University of Southern California, Spring 2020<br>-->
<!--	       Computer Graphics, University of Southern California, Fall 2019<br>-->
<!--	       Data Structures and Object Oriented Design, University of Southern California, Spring 2019<br>-->
<!--	       Advanced Digital Signal Processing, National Taiwan University, Spring 2017 <br>-->
<!--	       Differential Equation, National Taiwan University, Fall 2016 <br>-->
<!--	      </font> </p>-->
<!--	 </li>-->
	</ul>
	
<footer class="site-footer">
<br><br>
<div class="wrapper">
							<div class="footer-col">
							<a href="https://github.com/lxa9867" target="_blank">
							<img src="images/color-github.png" class="social-icon", width="40" height="40">
							</a>
<!--							<a href="https://twitter.com/ChoYingWu2" target="_blank">-->
<!--							<img src="images/color-twitter.png" class="social-icon", width="40" height="40">-->
<!--							</a>-->
							<a href="https://www.linkedin.com/in/xiang-li-40551a16a" target="_blank">
							<img src="images/color-linkedin.png" class="social-icon", width="40" height="40">
							</a>
							<a href="https://scholar.google.com/citations?hl=en&user=hGPBdf4AAAAJ" target="_blank">
							<img src="images/color-gscholar-footer.png" class="social-icon", width="40" height="40">
							</a>
							</div>
							</div>
						</div>

</footer>
		
</body></html>

